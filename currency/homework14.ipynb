{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1. 机器学习中的监督学习、非监督学习、强化学习有何区别\n",
    "- 监督学习有反馈，无监督学习无反馈，强化学习是执行多步之后才反馈。\n",
    "- 强化学习的目标与监督学习的目标不一样，即强化学习看重的是行为序列下的长期收益，而监督学习往往关注的是和标签或已知输出的误差。\n",
    "- 强化学习的奖惩概念是没有正确或错误之分的，而监督学习标签就是正确的，并且强化学习是一个学习+决策的过程，有和环境交互的能力（交互的结果以惩罚的形式返回），而监督学习不具备。\n",
    "#### （1） 监督学习即具有特征（feature）和标签（label）的，即使数据是没有标签的，也可以通过学习特征和标签之间的关系，判断出分类标签\n",
    "#### （2）无监督学习即只有特征，没有标签，只有特征，没有标签的训练数据集中，通过数据之间的内在联系和相似性将他们分成若干类——聚类。根据数据本身的特性，从数据中根据某种度量学习出一些特性。\n",
    "#### （3）强化学习与半监督学习类似，均使用未标记的数据，但是强化学习通过算法学习是否距离目标越来越近，我理解为激励与惩罚函数。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. 什么是策略网络，价值网络，有何区别\n",
    "- policy networks 主要是监督神经网络用于预测围棋的下一步，简单说是快速预测双方的下一手的位置，类似于棋手的第一感。实际上，策略网络的第一层是类似于这样的规则的集合,建立一个神经网络模型，它可以通过观察环境状态，直接预测出目前最应该执行的策略（policy），执行这个策略可以获得最大的期望收益（包括现在的和未来的reward）。和之前的任务不同，在强化学习中可能没有绝对正确的学习目标，样本的feature和label也不是一一对应。我们的学习目标是期望价值，即当前获得的reward和未来潜在的可获取的reward。所以在策略网络中不只是使用当前的reward作为label，而是使用Discounted Future Reward，即把所有未来奖励依次乘以衰减系数γ。这里的衰减系数是一个略小于但接近1的数，防止没有损耗地积累导致Reward目标发散，同时也代表了对未来奖励的不确定性的估计。\n",
    "- value-based networks 主要是预测在某个环境状态下所有行动的期望价值，然后通过选择 Q 值最高的行动执行策略。与策略网络不通的是Policy Based的方法相比于Value-Based，有更好的收敛性（通常可以保证收敛到局部最优，且不会发散），对高维或者连续值的Action非常高效（训练和输出结果都更高效），同时能学习出带有随机性的策略。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3. 请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select, Expansion，Simluation，Backpropagation是如何操作的\n",
    "- 选择(Selection):在选择阶段，需要从根节点，也就是要做决策的局面R出发向下选择出一个最急迫需要被拓展的节点N，局面R是是每一次迭代中第一个被检查的节点；对于被检查的局面而言，他可能有三种可能：\n",
    "\n",
    "    - 该节点所有可行动作都已经被拓展过\n",
    "    - 该节点有可行动作还未被拓展过\n",
    "    - 这个节点游戏已经结束了(例如已经连成五子的五子棋局面)\n",
    "- 拓展(Expansion)\n",
    "    - 在选择阶段结束时候，我们查找到了一个最迫切被拓展的节点N，以及他一个尚未拓展的动作A。在搜索树中创建一个新的节点Nn作为N的一个新子节点。Nn的局面就是节点N在执行了动作A之后的局面。\n",
    "- 模拟(Simulation)\n",
    "    - 为了让Nn得到一个初始的评分。我们从Nn开始，让游戏随机进行，直到得到一个游戏结局，这个结局将作为Nn的初始评分。一般使用胜利/失败来作为评分，只有1或者0。\n",
    "- 反向传播(Back Propagation)\n",
    "    - 在Nn的模拟结束之后，它的父节点N以及从根节点到N的路径上的所有节点都会根据本次模拟的结果来添加自己的累计评分。如果选择中直接发现了一个游戏结局的话，根据该结局来更新评分。每一次迭代都会拓展搜索树，随着迭代次数的增加，搜索树的规模也不断增加。当到了一定的迭代次数或者时间之后结束，选择根节点下最好的子节点作为本次决策的结果。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4.假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑\n",
    "- 新模块冷启动\n",
    "- 过滤同质化视频，确保用户所看视频不要过于单一\n",
    "- 推荐产品中插入商业广告主要需要考虑一下三个问题：\n",
    "\n",
    "    -（1）是否插入广告；\n",
    "    -（2）插入什么广告；\n",
    "    -（3）在什么位置插入广告。\n",
    "    \n",
    "利用深度强化学习来自适应的调整广告投放策略，在保证用户体验的基础上，最大化平台的收益\n",
    "\n",
    "### [Deep Reinforcement Learning for Online Advertising in Recommender Systems](https://arxiv.org/abs/1909.03602)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路\n",
    "自己没什么思路，找了一篇高引方法： [End to End Learning for Self-Driving Cars](https://arxiv.org/pdf/1604.07316.pdf)\n",
    "![](https://raw.githubusercontent.com/guanqin-123/kkb_notes/main/currency/currency_data/rldrive.png)\n",
    "#### 端到端自动驾驶核心：\n",
    "- 图像被送入一个卷积神经网络，然后计算一个被推荐的转向命令。这个被推荐的转向命令会与该图像的期望命令相比较，卷积神经网络的权重就会被调整以使其实际输出更接近期望输出。\n",
    "- 一旦训练完成，网络就能够从单中心摄像机（single center camera）的视频图像中生成转向命令。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Action（五子棋）：棋盘大小 10 * 10 采用强化学习（策略价值网络），用AI训练五子棋AI 请说明都有哪些模块，不同模块的原理\n",
    "- graphics模块：处理graph 的棋盘棋子位置\n",
    "- multiprocessing 多线程标记处理\n",
    "- torch or tf 模型框架\n",
    "- strategies.MCTSPlayer 蒙特卡罗标准库策略\n",
    "#### 1. RL 基本逻辑:\n",
    "![](http://download.broadview.com.cn/Original/18087c2139a8fccc978a)\n",
    "\n",
    "- 强化学习如上图，Agent 会根据环境给予的reward 调整action 的一个反馈系统，最终实现利益最大化，难点在于Agent 的行为通常会改变环境，而环境又会影响行为策略,具体到围棋上，这个策略的核心是根据围棋的特性：\n",
    "\n",
    "    - 在每一步双方信息完全已知；\n",
    "\n",
    "    - 每一步的策略只需考虑这一步的状态。\n",
    "#### 2. 马尔科夫决策中使用蒙特卡罗策略\n",
    "![](http://download.broadview.com.cn/Original/1808cbdfd2ea0de8ec9b)\n",
    "- 让机器来做就是有监督学习的回归算法，你要提取棋局的特征，算出对应每一个走法出现的概率P(a(t)|s(t))，然而围棋棋局的特征实在太复杂，这时候我们的深度学习开始派上用场，它可以自发地学习事物的表征\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}